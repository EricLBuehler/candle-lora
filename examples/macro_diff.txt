--- examples/linear_macro.rs	2023-09-15 13:57:38.483749288 -0400
+++ examples/linear.rs	2023-09-15 13:58:02.783750282 -0400
@@ -1,18 +1,34 @@
-use candle_core::{DType, Device, Module, Result, Tensor};
-use candle_lora::{LinearLayerLike, LoraConfig, LoraLinearConfig};
-use candle_lora_macro::{replace_layer_fields, AutoLoraConvert};
-use candle_nn::{init, Linear, VarMap};
+use std::{collections::HashMap, hash::Hash};
 
-#[replace_layer_fields]
-#[derive(AutoLoraConvert, Debug)]
+use candle_core::{DType, Device, Result, Tensor};
+use candle_lora::{
+    LinearLayerLike, Lora, LoraConfig, LoraLinearConfig, NewLayers, SelectedLayersBuilder,
+};
+use candle_nn::{init, Linear, Module, VarMap};
+
+#[derive(PartialEq, Eq, Hash)]
+enum ModelLayers {
+    Layer,
+}
+
+#[derive(Debug)]
 struct Model {
-    a: Linear,
-    _b: i32,
+    layer: Box<dyn LinearLayerLike>,
 }
 
 impl Module for Model {
     fn forward(&self, input: &Tensor) -> Result<Tensor> {
-        self.a.forward(input)
+        self.layer.forward(input)
+    }
+}
+
+impl Model {
+    fn insert_new(&mut self, new: NewLayers<ModelLayers>) {
+        for (name, linear) in new.linear {
+            match name {
+                ModelLayers::Layer => self.layer = Box::new(linear),
+            }
+        }
     }
 }
 
@@ -20,6 +36,7 @@
     let device = Device::Cpu;
     let dtype = DType::F32;
 
+    //Create the model
     let map = VarMap::new();
     let layer_weight = map
         .get(
@@ -32,21 +49,23 @@
         .unwrap();
 
     let mut model = Model {
-        a: Box::new(Linear::new(layer_weight.clone(), None)),
-        _b: 1,
+        layer: Box::new(Linear::new(layer_weight.clone(), None)),
     };
 
+    let mut linear_layers = HashMap::new();
+    linear_layers.insert(ModelLayers::Layer, &*model.layer);
+    let selected = SelectedLayersBuilder::new()
+        .add_linear_layers(linear_layers, LoraLinearConfig::new(10, 10))
+        .build();
+
     let loraconfig = LoraConfig::new(1, 1., None, &device, dtype);
-    model.get_lora_model(
-        loraconfig,
-        Some(LoraLinearConfig::new(10, 10)),
-        None,
-        None,
-        None,
-    );
+
+    let new_layers = Lora::convert_model(selected, loraconfig);
+
+    model.insert_new(new_layers);
 
     let dummy_image = Tensor::zeros((10, 10), DType::F32, &device).unwrap();
 
-    let digit = model.forward(&dummy_image).unwrap();
-    println!("Output: {digit:?}");
+    let lora_output = model.forward(&dummy_image).unwrap();
+    println!("Output: {lora_output:?}");
 }
